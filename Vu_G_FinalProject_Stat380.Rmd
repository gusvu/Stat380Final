---
title: "Vu Gustav Final Project Stat380"
author: Gustav Vu
output: html_document
date: "August 24, 2022"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Front Matter - Clean Environment, Load Libraries, User Defined Functions
```{r, include=FALSE}
rm(list = ls())
library(ggplot2)
library(dplyr)
library(tidyverse)
library(glmnet)
library(fastDummies)
library(FNN)
library(randomForest)
library(naivebayes)
library(psych)
```

## Load in and clean data
```{r}
#Data Set 1
CODGames_p1_380 <- read.csv("C:/Users/gusvu/OneDrive/Desktop/Stat 380/Proj/CODGames_p1_380.csv", stringsAsFactors=TRUE)
#This Data Set will represent Player 1

#Data Set 2
CODGames_p2_380 <- read.csv("C:/Users/gusvu/OneDrive/Desktop/Stat 380/Proj/CODGames_p2_380.csv", stringsAsFactors=TRUE)
#This Data Set Will Represent Player 2

#Combine Data Set 1 and Data Set 2
CODGames_p1_380 <- CODGames_p1_380 %>%
  mutate("Player" = "Player1")
CODGames_p2_380 <- CODGames_p2_380 %>%
  mutate("Player" = "Player2")
CODGames_380 <- rbind(CODGames_p1_380, CODGames_p2_380)
#remove old data from environment
rm(CODGames_p1_380)
rm(CODGames_p2_380)

#Data Set 3
CODGameModes <- read.csv("C:/Users/gusvu/OneDrive/Desktop/Stat 380/Proj/CODGameModes.csv", stringsAsFactors=TRUE)
#This Data Set has information about various types of games
```

# Task 1 (Exploratory Analysis)
I will try to Find what game mode is most likely to reach the score limit. But first I need to prep my data.

First Step is to create temporary data frames. Then I will Create a new variable called ScoreLim. ScoreLim will be 1 if the Score Limit is reached and 0 otherwise
```{r}

#when doing the data cleaning I made no distinction between the HC version of a game made. As it is more or less the same game, but slightly more realistic.
tempTDM <- CODGames_380 %>%
  filter(GameType == "TDM" | GameType == "HC - TDM") %>%
  mutate("Game" = "TDM") %>%
  summarize(Game, Result)
tempTDM <- tempTDM %>%
  mutate("ScoreLim" = ifelse(grepl('100', tempTDM$Result), 1, 0))

tempHardpoint <- CODGames_380 %>%
  filter(GameType == "Hardpoint" | GameType == "HC - Hardpoint") %>%
  mutate("Game" = "Hardpoint") %>%
  summarize(Game, Result)
tempHardpoint <- tempHardpoint %>%
  mutate("ScoreLim" = ifelse(grepl("250", tempHardpoint$Result), 1, 0))

tempKC <- CODGames_380 %>%
  filter(GameType == "Kill Confirmed" | GameType == "HC - Kill Confirmed") %>%
  mutate("Game" = "Kill Confirmed") %>%
  summarize(Game, Result)
tempKC <- tempKC %>%
  mutate("ScoreLim" = ifelse(grepl("65", tempKC$Result), 1, 0))

tempDomination <- CODGames_380 %>%
  filter(GameType == "Domination" | GameType == "HC - Domination") %>%
  mutate("Game" = "Domination") %>%
  summarize(Game, Result)
tempDomination <- tempDomination %>%
  mutate("ScoreLim" = ifelse(grepl("200", tempDomination$Result), 1, 0))

#Next I will join all the data sets and remove the temporary ones.
ScoreLimit <- rbind(tempTDM, tempHardpoint)
ScoreLimit <- rbind(ScoreLimit, tempKC)
ScoreLimit <- rbind(ScoreLimit, tempDomination)
rm(tempTDM)
rm(tempHardpoint)
rm(tempKC)
rm(tempDomination)

# Next I will remove all the rows in which Result is empty, as this will not count towards the probability of score limit being reached.
ScoreLimit <- ScoreLimit[ScoreLimit$Result != "", ]

#lets Take a look at our data before we graph it.
ScoreLimit%>%
  group_by(Game)%>%
  summarise(n = n(), mean = mean(ScoreLim), sd = sd(ScoreLim))

```

All 14 of the Domination games played have reached the Score Limit. This is because there is no Time limit. .97 is the proportion of hard point games played that have reached the score limit. Based on thees summary statistics We can say that Hard point is the most likely the most likely game to reach the score limit. I will now create a plot to visualize this data. If we leave out Domination, which has no time limit, Hardpoint has the highest proportion of score limit reached, and the lowest standard deviation.

## Data Visualization.
When I coded the DF Score Limit, I have a binary variable Titled ScoreLim. This Variable is equal to 1 if the score limit is reached and 0 if the score limit is not reached. I can group by Game, then plot the average of ScoreLim as a bar chart to display their proportions.
```{r}
ggplot(ScoreLimit) + 
  stat_summary(aes(x = Game, y = ScoreLim, fill = Game), 
               fun.y = mean, 
               geom = "bar")
```

# Task 2 (Inference)
a. Which predictors are associated with the TotalXP?
```{r}
# a list of all the variables are 
names(CODGames_380)
```

This is a list of all the variables in the data set. It is not really easy for us to say which ones effect totalxp. For example one gun does not give more xp than another, however a player may do more damage on average with a certain gun and therefore it may effect the totalxp.

I will use a lasso function to choose prediction variables for totalXP. But first I must clean my data. 
```{r}
temp <- CODGames_380[, -c(22, 23, 24, 25)]#remove those variables as they have only 1 level.

temp <- temp %>%
  mutate(XPType_ = ifelse(XPType == "", NA,
                          ifelse(XPType == "10% Boost", "10% Boost",
                                 ifelse(XPType == "Double XP + 10%", "Double XP + 10%","Double XP + 10%"))))
temp <- temp[,-14]

temp$Player <- as.factor(temp$Player)
```


I will remove columns 16 - 22 because they have many NA values, no code needed this is clear just by looking at the data (they would not be good predictors if many values are missing)
further more they are, variables that are correlated with game type. variables such as, captures, defuses, plants... ect are dependent on game time. So as long as game type is in our model that variable will acount for this.
I will taking out  columns 1, 2, 4, 14 because it is voting data and has nothing to do with game play. They are only data about what map players voted on.
I will take out the date variable as it was to complicated to model every single date. I considered converting it to month but decided to leave it out of my model because it is not something the player can control if they wanted to maximize their XP
I will take out result as each game has its own unique result and there are no trends
```{r}
#When Using Lasso, or a step wise function It will automatically remove my data of NA's. If I were to do this my data would be empty. So I will clean my data and get rid of columns with lots of NA values.
temp2 <- temp %>%
  na.omit()
temp2
rm(temp2)

head(temp)
temp <- temp[, -c(16,17,18,19,20,21,22)]

temp <- temp[, -c(1, 2, 4, 14)] 

temp <- temp[, -2]

temp <- temp[, -3]

#I will now convert all empty strings to NA
temp[temp == ''] <- NA

temp <- temp %>%
  na.omit()

temp <- temp %>%
  mutate(HC = ifelse(grepl("HC", temp$GameType), 1, 0))
#Create an Indicator for HC so I can Simplify my Gametype Variable
temp$GameType <- ifelse(grepl("TDM", temp$GameType), "TDM", 
                             ifelse(grepl("Domination", temp$GameType), "Domination",
                                    ifelse(grepl("Hardpoint", temp$GameType), "Hardpoint",
                                           ifelse(grepl("Kill Confirmed", temp$GameType), "Kill Confirmed", NA))
                                    )) #simplify Gametype Variable
```

# Lasso method for choosing variables
```{r}
#first separate our x's and y's
Xmat <- model.matrix(TotalXP ~ . , data=temp)[ ,-1]
y <- temp$TotalXP 

#create a training testing split
set.seed(321)#This is so that I can repeat this "Randomly" and get the same result
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)

X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]


```

I will plot MSE, with log lambda, when lambda increase the number of perimeters in our model decreases
```{r}

set.seed (12345)
cv.out <- cv.glmnet(x = X_mat_train, y = y_train, 
                    alpha = 1, stanardize = TRUE,
                    nfolds=10)
set.seed(NULL)
plot(cv.out)
#left line is the lowest cross validation lambda_min
#right is lambda_1se, for 1 std error, the biggest red dot, that is in the error bar range for error min

cv.out$lambda.min
cv.out$lambda.1se

```

We can see that we have an MSE of 157.378 if we use the minimum value lets explore this more.

looking at our plot the left line is the lowest cross validation lambda_min and the right is lambda_1se, for 1 std error, the biggest red dot, that is in the error bar range for error min

## Model selecting
```{r}
#Option 1: Pick the lambda that has less parameters.
bestlam1 <- cv.out$lambda.1se
#Predict the responses for the test set (use for MSE calc)
lasso.pred1 <- predict(cv.out , s = bestlam1,
                      newx = X_mat_test)
#Find the coefficients
lasso.coef1 <- predict(cv.out , s = bestlam1,
                      type = "coefficients")
bestlam1
lasso.coef1
```

The Lasso method gave us 2 possible models. 1 that has the minimum MSE, and one that has a  MSE that is 1 standard error away from the first MSE. I decided to use the 2nd option as that model will have less parameters. This is what the model would look like

$$\hat{TotalXP} = 2831.6493290 + (-960.1715706)ChoiceCrossroadsStrike + (-513.5864976)ChoiceMiamiStrike +(304.0025743)Eliminations + (6.6934066)Deaths + (0.8203335)Score + (0.2488407)Damage + (486.6437575)PrimaryWeaponMilano821 + (6772.7036007)XPType_Double XP + 10%$$

## b. Of the predictors associated with the response, select one of the predictors and explain the 
relationship between the predictor and TotalXP. (The predictor you select is up to you, but it 
should be one of the predictors associated with TotalXP.)

The variable I choose was Eliminations. As a player's elimination increases by 1, totalXP will increase by 304.0025743. This is according to our MLR model we created with lasso regression.

## Clean Enviroment before movimng on to task 3.
```{r}
rm(cv.out)
rm(lasso.coef1)
rm(lasso.pred1)
rm(X_mat_test)
rm(X_mat_train)
rm(Xmat)
rm(bestlam1)
rm(train_ind)
rm(y)
rm(y_train)
rm(y_test)

#temp is a clean version of my data so I will rename it (still keeping my old data just in case)
COD_clean <- temp
rm(temp)
```


# Task 3 (Prediction)
Research Question. What model is best for predicting XPType? The three methods I will use are K nearest nieghbors, Random Forest, and Naive Bayes.

## Data Prep
I will remove Choice and Primary weapon from the data set. We would have to create n-1 indicators for every level of those variables and it will just make our model too complicated.
```{r}
COD_clean <- COD_clean[, -c(1,8)]
```


## KNN
KNN or K nearest neighbors, takes a point, and finds the k number of nearest neighbors. It will take the average of thees k points. Then it will use the average of the k points to make a prediction.

## Data Prep
```{r}
temp <- COD_clean
#create dummy variables for categorical variables We need to do this so we can scale our data on the next step.
temp$Player <- ifelse(temp$Player == "Player1", 1, 0)
temp$FullPartial <- ifelse(temp$FullPartial == "Full", 1, 0)
temp <- dummy_cols(temp, select_columns = c("GameType"))
temp <- temp[, -14] #function made n indicators when we really need n-1 indicators
temp <- temp[, -7] # created dummy variables so we can remove gametype

xvars <- names(temp)[-8]
temp[ , xvars] <- scale(temp[ , xvars], center = TRUE, scale = TRUE)
```

Since KNN depended on distance it is important that we scale our data if we use this method.

## Testing / Traing Split
```{r}
#train/Test split
#We do this so that we can train our data with 80% of the points then validate that data with the other 20%
set.seed(321)
train_ind <- sample(1:nrow(temp),
                    floor(.8*nrow(temp)))
set.seed(NULL)

Train <- temp[train_ind,]
Test <- temp[-train_ind,]
```

since our data set is large 806 obs (after we cleaned it) We do not need to use cross fold validation a simple test/train split will work fine.

## Model Building.
I will create a vector to find out what value I should use for k. I will find the k with the highest accuracy.
```{r}
maxk <- 50
AC_vec <- rep(NA, maxk)

for(i in 2:maxk){
  knn<- knn(train = Train[, xvars, drop = FALSE],
                test = Test[, xvars, drop = FALSE],
                cl = Train$XPType_,
                k = i)
  
  pred <- knn[1:nrow(Test)]
  temp2 <- table(pred, Test$XPType_) # collums are truth
  AC_vec[i] <- (temp2[1,1] + temp2[2,2])/(temp2[1,1] + temp2[1,2] + temp2[2,1] + temp2[2,2])
  #this calculates the accuracy if collums are truth and rows are predictors

}

which.max(AC_vec)
```

we see that we have the highest accuracy with 3 neighbors

## New Model 
```{r}
knn <- knn(train = Train[, xvars, drop = FALSE],
           test = Test[, xvars, drop = FALSE],
           cl = Train$XPType_,
           k = 3)
pred <- knn[1:nrow(Test)]
temp2 <- table(pred, Test$XPType_)
temp2
AC_vec[3]
```

Looking at the table, with 3 neighbors, we correctly predicted 138 game types out of 162 on our traing set. This gives us an Accuracy of 0.8518519. This works pretty well but lets compare this model to other methods.

Notice we did not remove Train_ind. This way we can use the same training/testing data with all our models.
```{r}
rm(AC_vec)
rm(i)
rm(knn)
rm(maxk)
rm(pred)
rm(temp2)
rm(Test)
rm(Train)
rm(xvars)
COD_Clean_Scale <- temp
rm(temp)
```

# Random forest
Random forest will randomly select mtry number of variables and use that to make a split in a tree. ntree is the number of trees to grow.

Data Prep Testing training split.
Data prep is done, we will use the same testing training split for a more accurate comparison. We do not need to scale the data because random forest is not distance dependent like KNN.
```{r}
Train <- COD_clean[train_ind,]
Test <- COD_clean[-train_ind,]
```

## Variable importance
Not much data prep as this was already taken care off. I will set up a Random forest with all variables, mtry=3, and nrtree = 500. This is just so I can get an idea of what variables are important.
```{r}
set.seed(123) #set seed bc rf uses bootstrapping
rf <- randomForest(as.factor(XPType_) ~ ., data = Train, 
                    ntree = 500, mtry = 3, importance = TRUE)
set.seed(NULL)
```

## Variable importance
Before we start building a model lets take a look at variable importance. Since we are doing classification we want to look at the GINI index, and mean decrease accuracy. They are 2 different measurements that tell us the same thing. 
```{r}
varImpPlot(rf, n.var = 9)
```

We can see that TotalXP, Damage, Game Type, Eliminations, and score seem to be the best predictors. It is important to take out non important variables because random forest randomly selects variables in the model.

## Tuning
Using thees predictors I will create a matrix of 4 possible values of mtry and 3 possible values of ntree. The matrix will hold the accuracy of thees parameters.
```{r}
#assign constants and storage for loop outlet
mtry <- 4 
ntree = 3
tune_ <- matrix(NA, nrow = ntree, ncol = mtry)

#loop
for(i in 1:ntree){
  
  for(j in 1:mtry){
    #build model 
    set.seed(123) #set seed bc rf uses bootstrapping
    rf <- randomForest(as.factor(XPType_) ~ TotalXP + Eliminations + GameType + Damage + Score, data = Train, 
                    ntree = ifelse(i == 1, 500, ifelse(
                      i == 2, 1000, ifelse(i == 3, 1500, NA)
                    )), mtry = j, importance = TRUE)
    set.seed(NULL) # using nested if else statements to get the right value for ntree.


    #calculate Accuracy
    pred <- predict(rf, newdata = Test, type = "response")
    tune_[i,j]<- mean(pred == Test$XPType_)
  }
}
```

## Results
```{r}
tune_
```

We see that the that the highest accuracy correlates to ntree = 500, and mtry = 4
```{r}
set.seed(123) #set seed bc rf uses bootstrapping
rf <- randomForest(as.factor(XPType_) ~ TotalXP + Eliminations + GameType + Damage + Score, data = Train, 
                    ntree = 500, mtry = 4, importance = TRUE)
set.seed(NULL)
pred_prob <- predict(rf, newdata = Test, type = "prob")
pred <- predict(rf, newdata = Test, type = "response")
mean(pred == Test$XPType_)
table(pred, Test$XPType_)
```
The accuracy for knn is 0.8518519. Using Random forest we get a much better accuracy of 0.9074074. This is the better method but lets try one more method. We can also see what predictions we got right based on the confusion matrix.

```{r}
rm(i)
rm(j)
rm(tune_)
rm(mtry)
rm(ntree)
rm(pred)
rm(rf)
```


# Naive Bayes
Naive Bayes is a kind of classifier which uses the Bayes Theorem. It predicts membership probabilities for each class such as the probability that given record or data point belongs to a particular class. The class with the highest probability is considered as the most likely class. Naive Bayes uses bayes theorem and conditional probability. For example, what isd the probability of double xp + 10% given a player got 10 eliminations.

## Data Prep
Data prep is already done I will use the same testing training split to keep things consistent.
```{r}
Train <- COD_clean[train_ind,]
Test <- COD_clean[-train_ind,]
```

## Model Building
Although the VIF was used for a decision Tree I will still use it for Naive Bayes, as it still shows me the most important predictors.
```{r}
model <- naive_bayes(XPType_ ~ TotalXP + Eliminations + GameType + Damage + Score, data = Train)
plot(model)
```

Ploting our model sows us the probability of Double XP + 10% and 10%v boost given different values of our predictors.

Calculating Accuracy. 
```{r}
pred <- predict(model, newdata = Test)
table(pred, Test$XPType_)
mean(pred == Test$XPType_)
```

Our Accuracy is 0.7901235 using Naive Bayes this is the worst accuracy of the 3 models. Random forest had an accuracy of 0.9074074 and KNN had an accuracy of 0.8518519. It is clear that random forest had the highest accuracy and is therefore the best classification method. We can also see what predictions we got right based on the confusion matrix.
